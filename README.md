# Beyond Detection: Making Intrusion Detection Systems more Explainable

With the complexity and scale of systems available in computer networks, machine learning models are increasingly being used and developed to detect malicious traffic and prevent various types of cyberattacks. However, many of these models operate as "black boxes," providing insufficient information about how their decisions are made. This lack of transparency undermines the trust of network administrators, who need to understand and justify how traffic is classified as malicious and the actions taken. Furthermore, the absence of explanations can lead to difficult model maintenance and challenges in identifying and correcting errors, discouraging improvements. Therefore, it is essential that the models adopted for this task are explainable, allowing network specialists - even without machine learning experience - to interpret and evaluate the decisions of an automated system.

## ‚úçÔ∏è Proposal

This work proposes a study focused on the analysis of the explainability of machine learning models applied to the detection of cyberattacks. The idea is to investigate how understandable and intuitive these models are from the perspective of a network professional. To achieve this, different models and explainability techniques will be selected and evaluated, such as SHAP (SHapley Additive exPlanations) and other similar tools, such as LIME (Local Interpretable Model-agnostic Explanations) and Anchors, of which two out of three will be chosen for experimentation. In addition to these models, an explanation will be provided for professionals with little or no experience with concepts in the field of machine learning.  

Disclaimer: this is a undergraduate thesis project.  

## üìö Observations

Each project branch is a step/objective in the final thesis. The main branch is the one that contains the final version of the project, which is the one that will be presented to the committee.

## üìö Author
- Thiago Duvanel Ferreira